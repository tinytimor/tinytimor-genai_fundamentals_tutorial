{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc0b1626-16c2-4e01-a77f-c00bf6d51c14",
   "metadata": {},
   "source": [
    "### Introduction to Working with LLMs and LangChain in Python \n",
    "\n",
    "This Jupyter notebook consists of a structured tutorial for using different LLMs via API, such as OpenAI, Google Gemini, and Anthropic Claude, with LangChain in a Python Jupyter notebook. This tutorial will cover the importance of LangChain, how to set it up, and how to use it with these different LLMs.\n",
    "\n",
    "By [Stefan Lehman](https://medium.com/@stefanlehman2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb0a497-323c-4068-9c45-2308105acd10",
   "metadata": {},
   "source": [
    "\n",
    "### Introduction to LangChain\n",
    "\n",
    "\n",
    "#### Why use LangChain?\n",
    "\n",
    "The framework is designed to be model-agnostic, meaning it can work with a variety of LLM APIs, providing a flexible platform for developing diverse applications. \n",
    "\n",
    "More specifically, LangChain achieves this by abstracting the complexities of LLMs models, making it easier for developers to implement and leverage these technologies without needing in-depth knowledge of the underlying mechanics of these models.\n",
    "\n",
    "#### Key Features:\n",
    "\n",
    "- **Model-Agnostic**: Compatible with models from OpenAI, Cohere, Hugging Face, etc.\n",
    "- **User-Friendly**: Simplifies building model applications with LLMs. \n",
    "- **Versatile**: Suitable for diverse projects with LLMs. \n",
    "\n",
    "---\n",
    "\n",
    "As of May 16th, 2024, the LangChain ecosystem consists of the following: \n",
    "\n",
    "- **langchain**: Chains, agents, and retrieval strategies that make up an application's cognitive architecture.\n",
    "- **langchain-core**: Base abstractions and LangChain Expression Language.\n",
    "- **langchain-community**: Third party integrations.\n",
    "Partner packages (e.g. langchain-openai, langchain-anthropic, etc.):\n",
    "    - Some integrations have been further split into their own lightweight packages that only depend on langchain-core.\n",
    "- **langgraph**: Build robust and stateful multi-actor applications with LLMs by modeling steps as edges and nodes in a graph.\n",
    "- **langserve**: Deploy LangChain chains as REST APIs.\n",
    "- **LangSmith**: A developer platform that lets you debug, test, evaluate, and monitor LLM applications.\n",
    "\n",
    "References: \n",
    "- [Introduction to LangChain Ecosystem](https://python.langchain.com/v0.2/docs/introduction/)\n",
    "- [Free Code Camp - Beginner's Guide to LangChain](https://www.freecodecamp.org/news/beginners-guide-to-langchain/)\n",
    "- [What is LangChain- AWS](https://aws.amazon.com/what-is/langchain/)\n",
    "- [LangChain Building a Simple LLM App](https://python.langchain.com/v0.2/docs/tutorials/llm_chain/)\n",
    "- [LangChain Chatbot Tutorial](https://python.langchain.com/v0.2/docs/tutorials/chatbot/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "357723a9-a11e-4afe-9725-eac77148c844",
   "metadata": {},
   "source": [
    "### Installation\n",
    "Either run the command:\n",
    "\n",
    "run `!pip install -r ../requirements.txt` below\n",
    "\n",
    "OR\n",
    "\n",
    "run `!pip install langchain langchain-openai langchain-google-genai langchain-anthropic langchain-core langchain-experimental` below"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e95b9bfb-3cde-4b8a-a672-7432d58a43e4",
   "metadata": {},
   "source": [
    "---\n",
    "#### Example of Instantiating Model with OpenAI Using LangChain\n",
    "\n",
    "[LangChain OpenAI](https://python.langchain.com/v0.1/docs/integrations/platforms/openai/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ef98c1e0-a71a-4ef6-bb97-cad7c40bbc8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "import os \n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = ''\n",
    "\n",
    "openai_model = ChatOpenAI(model=\"gpt-3.5-turbo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ce1f29-51cc-4633-8978-ccaf0336e992",
   "metadata": {},
   "source": [
    "----\n",
    "#### Example of Instantiating Model with Google Gemini Using LangChain\n",
    "\n",
    "[LangChain Google GenAI](https://python.langchain.com/v0.1/docs/integrations/chat/google_generative_ai/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1ebe6a04-bb35-4b46-892b-459299418908",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "import os \n",
    "\n",
    "os.environ[\"GOOGLE_API_KEY\"] = ''\n",
    "\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "\n",
    "gemini_model = ChatGoogleGenerativeAI(model=\"gemini-pro\", convert_system_message_to_human=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "930dce2d-ec59-40d4-82e6-c5c9eb45ff7e",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "#### Example of Instantiating Model with Anthropic Using LangChain\n",
    "\n",
    "[LangChain Anthropic](https://python.langchain.com/v0.1/docs/integrations/platforms/anthropic/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2d61f377-50d2-4560-b3ab-fabe2f6d6662",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_anthropic import ChatAnthropic\n",
    "import os \n",
    "\n",
    "os.environ[\"ANTHROPIC_API_KEY\"] = ''\n",
    "claude_model = ChatAnthropic(model='claude-3-sonnet-20240229')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f013f68d-2ded-4c52-8893-a1e48f9be4dd",
   "metadata": {},
   "source": [
    "### Example Usage of these Different Models with the Same LangChain Framework\n",
    "\n",
    "The code below shows that in one line of code after instantiating the mdoel you can swap out different LLMs for different tasks. This makes developing with LangChain ideal because you can quickly prototype and develop solutions that use different types of models from different LLM orgnaizations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6f5f06b1-d6ff-4da1-9abd-28bcce4308dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_prompt = \"hi!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "94fa90c8-14cf-452d-b4d3-6d4dc6e2a00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    SystemMessage(content=\"Translate the following from English into Italian\"),\n",
    "    HumanMessage(content=user_prompt),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66fa5e3a-ede7-4673-bc25-91e485788c5d",
   "metadata": {},
   "source": [
    "#### Using OpenAI's GPT-3.5-Turbo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a55f9b5c-4570-4877-b800-1b403c6e1b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_response = openai_model.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8fa6b3b8-5a55-4b97-93f5-6ab55a9e5921",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI's response: \n",
      "\n",
      "content='Ciao!' response_metadata={'token_usage': {'completion_tokens': 3, 'prompt_tokens': 20, 'total_tokens': 23}, 'model_name': 'gpt-4', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None} id='run-2991b337-8423-4d61-9bd0-98514f966f67-0'\n"
     ]
    }
   ],
   "source": [
    "print(\"OpenAI's response: \\n\")\n",
    "print(openai_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd054ee1-a831-403e-8f93-87bf108d5981",
   "metadata": {},
   "source": [
    "#### Using Google's Gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "da327c62-bfe2-40e2-a7ab-b36205cf5f43",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda3/envs/testing/lib/python3.12/site-packages/langchain_google_genai/chat_models.py:343: UserWarning: Convert_system_message_to_human will be deprecated!\n",
      "  warnings.warn(\"Convert_system_message_to_human will be deprecated!\")\n"
     ]
    }
   ],
   "source": [
    "gemini_response = gemini_model.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d36dac91-8b0f-4ddb-837f-5d2bf7dae265",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gemini's response: \n",
      "\n",
      "content='Ciao!' response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': [{'category': 'HARM_CATEGORY_SEXUALLY_EXPLICIT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HATE_SPEECH', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HARASSMENT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_DANGEROUS_CONTENT', 'probability': 'NEGLIGIBLE', 'blocked': False}]} id='run-b9a7b99c-1cbb-4bcc-b72e-46274da863a7-0'\n"
     ]
    }
   ],
   "source": [
    "print(\"Gemini's response: \\n\")\n",
    "print(gemini_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "554eeb52-3055-43b6-ac6e-58079e423b1d",
   "metadata": {},
   "source": [
    "#### Using Anthropic's Claude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8c7b32bb-bb95-4eab-b501-e1a0d4e7f4d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "claude_response = claude_model.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "17a0af5e-b27f-4947-b318-a35499f8a91b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Claude's response: \n",
      "\n",
      "content='Ciao!' response_metadata={'id': 'msg_012HRnb7C29ZkfEUs4T8nWfK', 'model': 'claude-3-sonnet-20240229', 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'input_tokens': 17, 'output_tokens': 7}} id='run-80ed67c4-193f-44e3-b070-d1981a46dfc5-0'\n"
     ]
    }
   ],
   "source": [
    "print(\"Claude's response: \\n\")\n",
    "print(claude_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3024134c-0635-4191-88ea-b3c24773e6c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
